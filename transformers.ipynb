{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the necessary libraries and modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the basic building blocks of the Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k= d_model//num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask= None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    def split_heads(self,x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size,seq_length,self.num_heads,self.d_k).transpose(1,2)\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(batch_size,seq_length, self.d_model)\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length) :\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[: , 0::2] = torch.sin(position * div_term)\n",
    "        pe[ : , 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer( 'pe' , pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return  x + self.pe[: , :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward =  PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward (self, x, mask):\n",
    "        attn_output =  self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all the arcitecture together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads,num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  \n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self,src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt !=0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_lenght = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_lenght, seq_lenght), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded =self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer class combines the previously defined modules to create a complete Transformer model. During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.\n",
    "\n",
    "The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer modelâ€™s output through the following steps:\n",
    "\n",
    "Generate source and target masks using the generate_mask method.\n",
    "Compute source and target embeddings, and apply positional encoding and dropout.\n",
    "Process the source sequence through encoder layers, updating the enc_output tensor.\n",
    "Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\n",
    "Apply the linear projection layer to the decoder output, obtaining output logits.\n",
    "These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.685439109802246\n",
      "Epoch: 2, Loss: 8.554183959960938\n",
      "Epoch: 3, Loss: 8.484004020690918\n",
      "Epoch: 4, Loss: 8.42577838897705\n",
      "Epoch: 5, Loss: 8.366117477416992\n",
      "Epoch: 6, Loss: 8.29647159576416\n",
      "Epoch: 7, Loss: 8.21731948852539\n",
      "Epoch: 8, Loss: 8.136025428771973\n",
      "Epoch: 9, Loss: 8.05974006652832\n",
      "Epoch: 10, Loss: 7.974019527435303\n",
      "Epoch: 11, Loss: 7.893943786621094\n",
      "Epoch: 12, Loss: 7.806487560272217\n",
      "Epoch: 13, Loss: 7.732024192810059\n",
      "Epoch: 14, Loss: 7.643455982208252\n",
      "Epoch: 15, Loss: 7.561409950256348\n",
      "Epoch: 16, Loss: 7.4799885749816895\n",
      "Epoch: 17, Loss: 7.395391464233398\n",
      "Epoch: 18, Loss: 7.316530704498291\n",
      "Epoch: 19, Loss: 7.2368268966674805\n",
      "Epoch: 20, Loss: 7.160756587982178\n",
      "Epoch: 21, Loss: 7.080069541931152\n",
      "Epoch: 22, Loss: 7.004004955291748\n",
      "Epoch: 23, Loss: 6.926163673400879\n",
      "Epoch: 24, Loss: 6.84535026550293\n",
      "Epoch: 25, Loss: 6.772156238555908\n",
      "Epoch: 26, Loss: 6.697566986083984\n",
      "Epoch: 27, Loss: 6.622840881347656\n",
      "Epoch: 28, Loss: 6.554442882537842\n",
      "Epoch: 29, Loss: 6.4816460609436035\n",
      "Epoch: 30, Loss: 6.410065174102783\n",
      "Epoch: 31, Loss: 6.340725421905518\n",
      "Epoch: 32, Loss: 6.267592906951904\n",
      "Epoch: 33, Loss: 6.204010963439941\n",
      "Epoch: 34, Loss: 6.139004230499268\n",
      "Epoch: 35, Loss: 6.0723443031311035\n",
      "Epoch: 36, Loss: 6.000852108001709\n",
      "Epoch: 37, Loss: 5.935602188110352\n",
      "Epoch: 38, Loss: 5.867977142333984\n",
      "Epoch: 39, Loss: 5.806097507476807\n",
      "Epoch: 40, Loss: 5.742772579193115\n",
      "Epoch: 41, Loss: 5.689716815948486\n",
      "Epoch: 42, Loss: 5.618230819702148\n",
      "Epoch: 43, Loss: 5.557209014892578\n",
      "Epoch: 44, Loss: 5.499222755432129\n",
      "Epoch: 45, Loss: 5.443521022796631\n",
      "Epoch: 46, Loss: 5.375245571136475\n",
      "Epoch: 47, Loss: 5.319804668426514\n",
      "Epoch: 48, Loss: 5.261173725128174\n",
      "Epoch: 49, Loss: 5.204067707061768\n",
      "Epoch: 50, Loss: 5.1463093757629395\n",
      "Epoch: 51, Loss: 5.0916666984558105\n",
      "Epoch: 52, Loss: 5.031010627746582\n",
      "Epoch: 53, Loss: 4.979161262512207\n",
      "Epoch: 54, Loss: 4.923696994781494\n",
      "Epoch: 55, Loss: 4.871175289154053\n",
      "Epoch: 56, Loss: 4.8143792152404785\n",
      "Epoch: 57, Loss: 4.764062404632568\n",
      "Epoch: 58, Loss: 4.709810256958008\n",
      "Epoch: 59, Loss: 4.663999080657959\n",
      "Epoch: 60, Loss: 4.604974269866943\n",
      "Epoch: 61, Loss: 4.554468154907227\n",
      "Epoch: 62, Loss: 4.498730182647705\n",
      "Epoch: 63, Loss: 4.446930408477783\n",
      "Epoch: 64, Loss: 4.4027509689331055\n",
      "Epoch: 65, Loss: 4.345770835876465\n",
      "Epoch: 66, Loss: 4.295138359069824\n",
      "Epoch: 67, Loss: 4.246406555175781\n",
      "Epoch: 68, Loss: 4.198390007019043\n",
      "Epoch: 69, Loss: 4.144190788269043\n",
      "Epoch: 70, Loss: 4.096800327301025\n",
      "Epoch: 71, Loss: 4.053375244140625\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr= 0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
